#+TITLE: Abcird

Abcird (pronounced /absurd/) is a proposed programming language for compiling
programs down to zkSNARKs.

Currently, abcird is aiming to support Plonk, therefore we might use the
following terms interchangeably:
- underlying proving systems, of which the current state of the art are
- SNARKs, which in the context of zero-knowledge are referred to as
- zkSNARKs, of which the current state of the art are
- Plonk instances, which are often abbreviated as
- Plonk, when referring to the chosen Plonk instance.

zkSNARKs, including Plonk, model the computation that we want to prove using
zero-knowledge as
- arithmetic circuits, which sometimes are sometimes referred to as
- zero-knowledge circuits, and altogether abbreviated as
- circuits.

We assume the basic knowledge of how zero-knowledge, including undestanding the
roles of prover and verifier. We write /witness/ or /witness vector/ to denote
the private inputs to the zero-knowledge proof and we write /statement/ or
/statement vector/ to denote the public inputs to the zero-knowledge proof. We
write /input/ or /input vector/ for the input to either prover or verifier,
meaning /witness/ together with /statement/ for the prover and /statement/ for
the verifier. Since the word "statement" often has a particular meaning in the
context of programming languages we prefer to refer to it as /input/ when
discussing verifier.

We do not assume any prior knowledge about circuits, but we do point out that in
our settings their are finite and bound by a constant.

We do not assume any prior knowledge about Plonk instances, but we do point out
that two Plonk instances are not compatible and can be treated as two separate
zkSNARKs.

Main contributions of this report are
- giving scenarios of transitions :: in Abcird and what they entail ([[* Scenarios
  of Transitions]]).
- ease the transition between underlying proving systems :: when changing
  between incompatible underlying proving systems ([[* Transition Between Proving
  Systems]]). In the near term that means a transition between two Plonk
  instances, however it could also mean transitioning to a completely different
  zkSNARK or transitioning to a completely different scheme of proving systems.
- provide a high-level programming language :: for writing programs which
  results need to be proven in zero-knowledge ([[* Abcird Programming Language]]).
  In the near term such programming language will be used by the Lares project
  for writing private smart contracts, however it could also be used by a larger
  Plonk community or retrofitted to a different underlying proving system.

* Bytecode, Versioning, Backwards Compatibility, and Hard-Forks.
There is an interesting relation between having a versioned bytecode and
hard-forks, where hard-fork becomes more of a means to ensure that all the
network has upgraded to a particular version.

Modifications to bytecode will probably result in hard-forks.

** /Thomas/: upgrade assumptions
I've been operating on the following some unspoken assumptions. First of these is:

/The dominant cost of compilation for the network is the verifier key generation./

This seems reasonable as most smart-contracts are small programs, with dominant
costs likely lying in the primitives used, and the fact that verifier key
generation requires moderately expensive group operations to be performed for
each gate in a source arithmetic circuit.

As a result, I believe that, where an upgrade invalidates old verifier keys,
compilation cost of Abcird should not be a concern, and it makes sense to
recompile from (close to) source.

In particular, I believe that we will identify an intermediate language of
Abcird, which is can both be targeted for any high-level language we may
eventually support, but also which can in turn target any ZK architecture we may
wish to switch to. I believe that we should use such an intermediate form as the
persistent contract bytecode, and ensure that the language capabilities
monotonically increase.

Any upgrade which invalidates verifier keys then would require re-running the
backend compilation pipeline for affected circuits, and as a consequence it also
makes sense to be proof-system specific in the VM bytecode, as this is /not/
persistent data, and is recompiled if verifier keys are invalidated.

I oppose a generic VM bytecode design, as this, in my opinion, pushes the entire
compiler backend into the VM bytecode. This bytecode would also be harder to /get
right/, as we'd have to define the right level of abstraction, rather than
selecting the right level of abstraction /for a specific upgrade/ when necessary
in our multi-pass Abcird compiler.

* Scenarios of Transitions
We will go over informal scenarions of changes to Abcird.  We assume some that
there is some starting instance of Plonk.

** Adding a hand-crafted gadget
Let's assume that someone writes an Abcird code that contains implementation of
novel hashing function of FooHash.  That function becomes popular and that there
are multiple deployed smart contracts that are using FooHash.

As FooHash gains popularity, someone figures out how to make FooHash more
efficient by using a hand-crafted gadget that targets the current Plonk
instance, which leads to FooHash2.

Newly written smart contracts can make use of FooHash2, but developers are free
to use the old version FooHash, as it is still working.

This one will not need a hard-fork.

** Adding X to Abcird VM (or ZK-Garage/Plonk)
Let's assume that FooHash becomes widely popular, but it suffers greatly from
the computational cost of computing the input vectors in Abcird VM, so a
specialised Rust subroutine is added to Abcird VM or ZK-Garage/Plonk.

This update will probably add a new instruction to bytecode, meaning that in
order to use the faster subroutine the old smart-contracts need to be
re-compiled.

However, old smart contracts are still fully backwards compatible.

Still, will probably warrant a hard-fork to ensure that everyone can support it.

** Changing X in Abcird VM (or ZK-Garage/Plonk)
There is FooHash in Abcird and we want to switch to FooHash2.

This is probably a circuit incompatible or input incompatible change, so we
probably would want to add it as a separate instruction.

In both cases it will probably warrant a hard-fork.

We need to be very vigilant about ZK-Garage/Plonk not doing that by accident and
hitting us by surprise.

** Removing X from Abcird VM (or ZK-Garage/Plonk)
Depracating bytecode instructions probably warrants a hard-fork.

** Adding a custom gate to Plonk instance
This is primarly a circuit incompatible change, meaning that all
- prover keys,
- verifier keys, and
- proofs
will stop working and need to be regenerated.

Needs a hard-fork.

** Switching to a completely different proving system alltogether
If we are lucky it will be bytecode compatible, if we are not lucky it might not
even be source code compatible.

Needs a hard-fork.

* Transition Between Proving Systems

The compilation pipeline for Abcird has the following elements
- an abcird program :: which is compiled by the abcird compiler to
- an abcird bytecode :: which is then interpreted by abcird vm to generate
  - compiled circuits :: in the form of
    - prover key :: and
    - verifier key :: .

then, during execution the abcird bytecode is interpreted by abcird vm to
produce
 - input vectors ::
   - witness vector :: and
   - statement vector :: .

In order to prove the abcird vm takes
- prover key,
- witness vector, and
- statement vector
in order to produce a proof.

In order to verify the abcird vm takes
- proof,
- verifier key, and
- statement vector
in order to verify the proof.

Therefore there are these distinct steps
- bytecode compilation
- circuit compilation
- input generation
- proof generation/verification

Therefore there are the following levels of compatibility
- source code compatible :: which is the weakest level of compatibility and does
  not provide any guarantees,
- bytecode compatible :: meaning that we do not need to recompile the bytecode,
- input compatible :: meaning that we do not need to regenerate the input,
- circuit compatible :: meaning that we do not need to recompile the circuit,
- proof compatible :: meaning that we do not need to regenerate the proof.

Proof compatibility is the most restrictive and essentially tells if we are
confident that we can still verify old proofs.  Interestingly, a change that is
not input compatible, but is circuit compatible, could potentially be proof
compatible, as long as we keep around the old statement vectors.

** Input Compatibility vs Circuit Compatibility
There is an interesting relation between input compatibility and circuit
compatibility.

Firstly, it is very easy to imagine that circuit compilation occurs before input
generation because circuit compilation feels like a part of a compilation
pipeline, whereas input generation is distinctly a runtime operation because we
need the inputs to the abcird program so we can generate the input to the prover
or verifier.

Still, input generation is independent to circuit compilation, and theoretically
leads to the following ordering
#+begin_src artist
                 +----------------------+
                 |source code compatible|
                 +----------------------+
                             |
                             |
                             |
                  +--------------------+
                  |byte code compatible|
                  +--------------------+
                            /\
               *-----------*  *-----------*
              /                            \
    +------------------+          +----------------+
    |circuit compatible|          |input compatible|
    +------------------+          +----------------+
              \                            /
               *-----------*  *-----------*
                            \/
                    +----------------+
                    |proof compatible|
                    +----------------+
#+end_src
meaning that some updates or modifications could be
- circuit compatible but not input compatible, or
- input compatible but not circuit compatible.

In practice, we so far encountered the latter (for example, when switching to a
plonk instance with an additional custom gate) and we are not sure if the former
is even possible, as input compatibility seems to be implied by the circuit
compatibility.

* Abcird Programming Language

A compilation of an abcird program will involve the following tools:
- abcirdc :: a compiler written in Chez Scheme that compiles programs written in
  abcird programming language down to abcird bytecode, and
- abcird-vm :: a vm/runtime written in Rust that
  - compiles abcird bytecode to a compiled form that is understood by the
    underlying proving system, and
  - interpretes abcird bytecode to generate input vectors for the underlying
    proving system.

When working with abcird there will be following artifacts:

- abcird programs :: high-level programs written in abcird programming language,
- abcird bytecode :: abcird programs compiled by the abcird compiler,
- compiled circuits :: abcird bytecode compiled down to the form understood by
  the underlying proof system,
- input vectors :: computed input to the prover system.

** Mutability
For the time being we assume immutable variables and we might add some form of
mutability later.  Currently mutability is restricted to oracles, as oracles can
contain mutable state themselves.

** Modules
We want to add compile-time parameters to functions and types, for instance we
want to parameterize functions operating on merkle trees and merkle paths by the
path lenght, which will be known at compile time.

We are looking for module systems of the following programming
languages
- Rust, or
- ML-family languages
to look for the inspiration.

Due to the restrictions of the abcird language, modules are compile-time
constructs, and we believe that they can be fully desugared during the
compilation pipeline.

We also believe that modules will allow us to parameterize our code but still
have a nominal type system.

*** Module and Structs instead of Classes
We started with a language that looks like a TypeScript, however we will
probably go more towards Rust design space, because we conjecture that modules +
structs + methods defined on structs will map nicer to the restrictions of the
underlying language rather than classes.

Syntactically modules can look similar to classes, however semantically they are
similar to static class factories (producing static classes themselves).
Modules are similar to static classes.

*** Proposed Module Syntax
#+begin_src text
module <module_name>[<module parameters...>] {
       <module members...>
}

using <module_name>[<parameter instantiation>];
using <module_name>[<parameter isntantiation>] as <module_alias>;
#+end_src

*** Example of Desugaring a Module:  Merkle Path Library
We start by writing a module for operating on merkle paths using our module
syntax

#+begin_src text
module merkle_path[n: Natural] {
  struct merkle_root { <...> };
  struct merkle_path_entry { <...> };
  struct merkle_path { <...> };
  circuit merkle_path_root(path: merkle_path[n]): merkle_root {
    <...>
  };
}
#+end_src

Then we can desugar the module into a prefix on circuit and structures
#+begin_src text
struct merkle_path$merkle_root[n: Natural] { <...> };
struct merkle_path$merkle_path_entry[n : Natural] { <...> };
struct merkle_path$merkle_path[n : Natural] { <...> };
circuit merkle_path$merkle_path_root[n : Natural](path: merkle_path$merkle_path[n]): merkle_path$merkle_root[n] {
  <...>
};
#+end_src

Finally, at the inport site, we can instantiate =n= and add it to the prefix
(here we instantiate =5= for =n=):
#+begin_src text
struct merkle_path`5$merkle_root { <...> };
struct merkle_path`5$merkle_path_entry { <...> };
struct merkle_path`5$merkle_path { <...> };
circuit merkle_path`5$merkle_path_root(path: merkle_path`5$merkle_path): merkle_path`5$merkle_root {
  <...>
};
#+end_src

** Interfacing with Oracles
Oracles are implemented outside of abcird, therefore we need to define a way for
interfacing with them. We also realised that we need to capture /external
types/, which are defined inside oracles, but which do not mean anything in
abcird. One approach to do so is the use of /existential types/ from ML-family
languages.

Existential types are used widely in OCaml, however the UX of using them might
be foreign to smart contract developers:
- developers with TypeScript experience might be confused by existential types,
  because TypeScript does not have support for existential types.
- developers with Rust experience might be familiar with existential types,
  because Rust has similar construct in the form of Traits' associated types.

Still, we need to be wary about uncanny valley of using existential types, as
developers tend to work with either concrete types or type reflection, which
allows them to inspect the type during runtime.

*** Example:  Existential Types in OCaml
#+begin_src text
type merkle_root = {
  bytes : byte array;
};

module merkle_path :
    sig
      type merkle_path;
      val merkle_path_root(path: t): merkle_root;
    end =
    struct
      type merkle_path = {
           ...
        }
    end

module type PUBLIC =
    sig
      type t;
      val topic : t option;
      val set_topic : t -> unit;
    end
#+end_src

*** Potential Syntax for Specifying Oracles as Interfaces (Module Types)
One way to specify oracles is to specify them as /interfaces/, which are
collection of types bundled together, roughly resembling module types from
OCaml.

Potential example of specifying public and private oracle
#+begin_src text
public interface public_oracle {
  type topic;
  statement hash(topic: topic): Field;
  statement topic(): Maybe[topic];
  statement topic$set(topic: topic): Void;
}
#+end_src

** TODO Hashing Existential Types
Do we need to autogenerate the code for hashing existential types, or should we
always defer that to the oracles.

** TODO Entry Point/Main Module
We need to figure out how we will mark the entry point to abcird programs.
Should that be a distinguished module that will expose functions that will
become transition functions of the deployed smart contract?  Or should that be a
single function that will pattern match on it.

** Compilation of Abcird programs
Compiling programs down to zkSNARKs usually poses an interesting challenge: not
only we need to generate a circuit for the c

** Compilation Artifacts vs Evaluation Artifacts

* Abcird low level target (PlonkVM)
** API endpoints
We probably want something similar to the following C-style API to be
accessible, although it may be split between proof server, verifiers, and
witness generation.

#+begin_src C
#include <stdint.h>
typedef uint8_t[48] field_t;
struct error_t {
  uint64_t code;
  const uint8_t* message;
}
struct prover_key {
  uint64_t length;
  const uint8_t* data;
}
struct verifier_key {
  uint64_t length;
  const uint8_t* data;
}
struct keypair {
  prover_key prover;
  verifier_key verifier;
}
struct witness_vector {
  uint64_t length;
  const field_t* data;
  uint64_t statement_length;
  const field_t** statement_indicies;
}
struct statement_vector {
  uint64_t length;
  const field_t* data;
}
struct reference_string { ... }
struct proof { ... }
void register_external(uint64_t index, uint64_t argument_size, uint64_t output_size, error_t (*fn_ptr)(const field_t* data, field_t* output));
error_t compile_keys(uint64_t prog_length, const uint8_t* prog, reference_string srs, keypair* output);
error_t compile_prover_key(uint64_t prog_length, const uint8_t* prog, reference_string srs, prover_key* output);
error_t compile_verifier_key(uint64_t prog_length, const uint8_t* prog, reference_string srs, verifier_key* output);
error_t witness_generate(uint64_t prog_length, const uint8_t* prog, uint64_t arg_length, const field_t* args, witness_vector* output);
void witness_to_statement(witness_vector wit, statement_vector* output);
error_t prove(prover_key key, witness_vector witness, proof* output);
error_t verify(verifier_key key, statement_vector stmt, proof pi);
#+end_src

** Low-level target
The low-level target should be a sequence of bytecode instructions. The
following are some operations already supported by Plonk, which we likely want
to expose:

| Name                    | Inp | Out | Imm   | Desc                                                                                    |
|-------------------------+-----+-----+-------+-----------------------------------------------------------------------------------------|
| =pure_gate=               |   3 |     | 5×F   | Calls =poly_gate= on pre-existing variables and coefficients                              |
| =pure_gate_pub_input=     |   3 |     | 6×F   | Like =pure_gate= with non-zero =pi=                                                         |
| =pure_gate_out=           |   2 |   1 | 4×F   | Calls =poly_gate= with =q_o = 1= and =c= being a generated variable                           |
| =pure_gate_out_pub_input= |   2 |   1 | 6×F   | Like =pure_gate_out= with non-zero =pi=                                                     |
| =constrain_to_boolean=    |   1 |     |       | Calls =boolean_gate=                                                                      |
| =cond_select=             |   3 |   1 |       | Calls =conditional_select=                                                                |
| =load_imm=                |     |   1 | 1×F   | Calls =add_input= and =constrain_to_constant=                                               |
| =declare_pub_input=       |   1 |     |       | Calls (ironically) =constrain_to_constant(a, Zero::zero(), Some(a.value()))= (pseudocode) |
| =constrain_eq=            |   2 |     |       | Calls =assert_equal=                                                                      |
| =test_eq=                 |   2 |   1 |       | Calls =is_eq_with_output=                                                                 |
| =poseidon_compress=       |   2 |   1 |       | Calls =Poseidon<...>::output_hash=                                                        |
| =ecc_add=                 |   4 |   2 |       | Calls =point_addition_gate=                                                               |
| =ecc_generator_mul=       |   1 |   2 |       | Calls =fixed_base_scalar_mul=                                                             |
| =ecc_mul=                 |   3 |   2 |       | Calls =variable_base_scalar_mul=                                                          |
| =call_extern=             |   n |   m | 1×u64 | Calls externally registered function pointer (indexed by immediate)                     |
| =call_extern_guard=       | n+1 |   m | 1×u64 | As =call_extern=, but only if the first input is non-zero                                 |

- Each instruction may be represented as a sequence of bytes, and operate on an unbounded append-only array of intermediate values (the stack).
- The stack first 8 bytes of a program LE-encode a =u64= representing the number of inputs the program. These are modelled as pre-existing variables on the stack.
- The first byte identifies the instruction.
- Inputs are provided as indexes onto the stack, each being represented as a LE-encoded 64-bit unsigned integer, with indexes starting at zero.
- The stack grows by the output amount, and each value is a field element.
- Immediates may be either field elements, which are represented as a LE-encoded sequence of 48 bytes (for BLS12-381), or another fixed constant, or LE-encoded =u64=.
- Each instruction, except =call_extern= consists of the instruction opcode, any input indicies it takes, and any immediates it takes.
- =call_extern= and =call_extern_guard= begin with a LE-encoded =u64=, identifying a previously registered external handle (with =register_external=). For =call_extern_guard=, the next argument refers to the variable controlling if the call will be performed. The following =n = argument_size= LE encoded field elements are identified dynamically, using the recorded size of the argument vector for this =call_extern=.

* 2022-04-13 Lares Weekly Meeting
Plonk is more of a framework to create zkSNARKs.

We actually talk about Plonk Instances.

If you recall for proving

- prover key (compiled form of the circuit),
- prover input vector (public and private parts encoded as a vector of field
  elements),

For verification

- proof (output from the prover),
- verifier key (compiled for of the circuit),
- verifier input vector (public parts encoded as a vector of field elements).

Where does the input generation come from?

If I have a function =foo= and I want to create a ZK proof that =x = foo(y)=, I
need to encode =foo= as a ZK circuit and that circuit will contain some
additional variables (inputs).

Assume that I can encode =x= and =y= into a single field element.  That means
that I will have at least 2 public inputs (one for =x= and one for =y=).

I take the code/semantics/meaning of =foo= and encode it to a circuit with
additional elements. For intance, each division will contain at least one extra
private input. Usually there are a lot of private inputs, for instance hashing
functions can contain up to 1000s of private inputs/intermediate values.

And all of these intermediate values consume the budget for circuits.

Scenarios we were discussing
- there is hashing function in =zk-garage/plonk= and someone comes up with a
  improved/fixed version of that function.
- 3rd party comes up with a hashing function (or any function that could be used
  by a wide variety of smart contracts)
  - we could provide a Rust implementation of the input generation part,
  - we could provide a hand-crafted circuit (or gadget) for that particular function.

Privacy in zkSNARKs:
- proofs are succint (so small that they cannot contain private information),
- ECCs (you add randomness to your proof, making it exteremly difficult to
  extract private information from the proof).

Because we need to do verification on the node and we do not want to ship Rust
compiler as a part of the node, we realised that we need something similar to an
interpreter that can handle proof verification.

Instead of having 3 separate compilation artifacts, we decided to have a single
compilation artifact, that can be interpreted in 3 different ways.

Why verifier needs that bytecode?  Depends on what are your trust boundaries.
And also it depends on who is responsible for handling encoding of values to
field elements?

So far we have compatibilities
- source-level compatibility :: we can still compile it,
- byte-code comaptibility :: we don't need to recompile the code,
- circuit compatibility :: we don't need to regenerate prover/verifier keys and
  inputs,
- proof compatibility :: we don't need to regenerate the proofs.

** Assumptions
- we deploy smart contracts as bytecode.

** Questions
- what is our story for a snark upgrade?
